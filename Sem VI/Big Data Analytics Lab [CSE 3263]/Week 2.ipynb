{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Implement a PySpark script that applies transformations like filter and withColumn on a\n",
    "DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession,functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-------+------+\n",
      "|roll|   name |country|marks |\n",
      "+----+--------+-------+------+\n",
      "|  57| saksham|    UAE|    12|\n",
      "|  45|  arjun |    IND|    34|\n",
      "|  40|rishabh |    BAN|    56|\n",
      "|  14|  raman |    IND|    89|\n",
      "|  36|  rishi |    USA|     1|\n",
      "+----+--------+-------+------+\n",
      "\n",
      "+----+--------+-------+------+\n",
      "|roll|   name |country|marks |\n",
      "+----+--------+-------+------+\n",
      "|  57| saksham|    UAE|    12|\n",
      "|  45|  arjun |    IND|    34|\n",
      "|  40|rishabh |    BAN|    56|\n",
      "|  36|  rishi |    USA|     1|\n",
      "+----+--------+-------+------+\n",
      "\n",
      "+----+--------+-------+------+\n",
      "|roll|   name |Country|marks |\n",
      "+----+--------+-------+------+\n",
      "|  57| saksham|    USA|    12|\n",
      "|  45|  arjun |    USA|    34|\n",
      "|  40|rishabh |    USA|    56|\n",
      "|  14|  raman |    USA|    89|\n",
      "|  36|  rishi |    USA|     1|\n",
      "+----+--------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create Spark Session \n",
    "spark=SparkSession.builder.appName('dataframe_transforms').getOrCreate()\n",
    "df=spark.read.csv('week2data.csv',header=True,inferSchema=True)\n",
    "df.show()\n",
    "\n",
    "#filter\n",
    "df.filter(df[\"Roll\"]>14).show()\n",
    "\n",
    "#withColumn\n",
    "df2=df.withColumn(\"Country\",lit(\"USA\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write a PySpark script that performs actions like count and show on a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the dataframe is 5\n",
      "The number of entries with the name starting with \"a\" is: 2 entries\n",
      "+----+------+-------+------+\n",
      "|roll| name |country|marks |\n",
      "+----+------+-------+------+\n",
      "|  45|arjun |    IND|    34|\n",
      "|  14|raman |    IND|    89|\n",
      "+----+------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"The length of the dataframe is\", df.count())\n",
    "df_mod = df.filter(df.country.startswith('I'))\n",
    "print('The number of entries with the name starting with \"a\" is: ' + str(df_mod.count()) + ' entries')\n",
    "df_mod.show()\n",
    "\n",
    "#Might give an error if the file does have whitespaces in the cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Demonstrate how to perform basic aggregations (e.g., sum, average) on a PySpark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum marks\n",
      "+-----------+\n",
      "|max(Marks )|\n",
      "+-----------+\n",
      "|         89|\n",
      "+-----------+\n",
      "\n",
      "The average marks:\n",
      "+-----------+\n",
      "|avg(Marks )|\n",
      "+-----------+\n",
      "|       38.4|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum as sum1\n",
    "from pyspark.sql.functions import avg \n",
    "from pyspark.sql.functions import max as max1\n",
    "\n",
    "print(\"The maximum marks\")\n",
    "df.select(max1(\"Marks \")).show()\n",
    "print(\"The average marks:\")\n",
    "df.select(avg(\"Marks \")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
