{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Resolution Application using PySpark "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\"> 210962204\n",
    "Kumud \n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " donation   'WEEK 1.ipynb'  'Week 2.ipynb'\r\n",
      " week1.csv   week2data.csv  'Week 3.ipynb'\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.executor.memory\", \"3g\") \\\n",
    "    .config(\"spark.driver.cores\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "!ls\n",
    "df = (\n",
    "    spark.read.csv('donation/block*.csv', header=True, inferSchema=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "|     cmp_fname_c1|cmp_fname_c2|cmp_lname_c1|cmp_lname_c2|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|\n",
      "+-----------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "|0.833333333333333|        null|         1.0|        null|      1|     1|     1|     1|      0|    true|\n",
      "|                1|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|                1|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|                1|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|                1|        null|         1.0|           1|      1|     1|     1|     1|      1|    true|\n",
      "|                1|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|                1|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|                1|        null|         1.0|        null|      1|     1|     1|     1|      0|    true|\n",
      "|                1|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|                1|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|                1|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|                1|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|                1|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|                1|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|                1|        null|         1.0|        null|      1|     0|     1|     1|      1|    true|\n",
      "|                1|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|                1|           1|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|                1|           1|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|                1|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "|                1|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|\n",
      "+-----------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max, avg, min, count, when\n",
    "df = df.replace({'?': None})\n",
    "df = df.drop(df.id_1).drop(df.id_2)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+\n",
      "|summary|      cmp_fname_c1|       cmp_fname_c2|\n",
      "+-------+------------------+-------------------+\n",
      "|  count|           5748125|             103698|\n",
      "|   mean|  0.71290247044295| 0.9000176718903214|\n",
      "| stddev|0.3887583596162793|0.27131761057823345|\n",
      "|    min|                 0|                  0|\n",
      "|    max|                 1|                  1|\n",
      "+-------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Develop a PySpark script to clean and preprocess data before performing entity resolution.Include steps like tokenization and normalization.\n",
    "#Cleaning data: drop null values, retain columns with high amount of data, normalize integer values\n",
    "df = df.drop('id_1')\n",
    "df = df.drop('id_2')\n",
    "summary = df.describe()\n",
    "#summary of columns \n",
    "summary.select(\"summary\", \"cmp_fname_c1\", \"cmp_fname_c2\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+-------------------+-------------------+-------------------+--------------------+--------------------+-------------------+-------------------+\n",
      "|summary|       cmp_fname_c1|       cmp_fname_c2|       cmp_lname_c1|       cmp_lname_c2|            cmp_sex|              cmp_bd|              cmp_bm|             cmp_by|            cmp_plz|\n",
      "+-------+-------------------+-------------------+-------------------+-------------------+-------------------+--------------------+--------------------+-------------------+-------------------+\n",
      "|  count|              20922|               1333|              20931|                475|              20931|               20925|               20925|              20925|              20902|\n",
      "|   mean| 0.9973163859635041| 0.9898900320318176| 0.9970152595958817| 0.9693701678438521|  0.987291577086618|  0.9970848267622461|  0.9979450418160095| 0.9961290322580645| 0.9584250310975027|\n",
      "| stddev|0.03650667584833679|0.08251973727615237|0.04311880753394513|0.15345280740388917|0.11201570591216435|0.053914876598079815|0.045286127452170664|0.06209804856731054|0.19962063345931916|\n",
      "|    min|                  0|                  0|                0.0|                  0|                  0|                   0|                   0|                  0|                  0|\n",
      "|    max|                  1|                  1|                1.0|                  1|                  1|                   1|                   1|                  1|                  1|\n",
      "+-------+-------------------+-------------------+-------------------+-------------------+-------------------+--------------------+--------------------+-------------------+-------------------+\n",
      "\n",
      "+-------+-------------------+------------------+-------------------+-------------------+------------------+------------------+------------------+------------------+--------------------+\n",
      "|summary|       cmp_fname_c1|      cmp_fname_c2|       cmp_lname_c1|       cmp_lname_c2|           cmp_sex|            cmp_bd|            cmp_bm|            cmp_by|             cmp_plz|\n",
      "+-------+-------------------+------------------+-------------------+-------------------+------------------+------------------+------------------+------------------+--------------------+\n",
      "|  count|            5727203|            102365|            5728201|               1989|           5728201|           5727412|           5727412|           5727412|             5715387|\n",
      "|   mean| 0.7118634802167386|0.8988473514090184| 0.3131380113359623|0.16295544855122535|0.9548833918362851|0.2216425149788421| 0.486995347986141|0.2199230647280133|0.002043781112285135|\n",
      "| stddev|0.38908060096985375|0.2727209029401023|0.33228121305726965| 0.1930236663528703|0.2075598885921742|0.4153518275558718|0.4998308940493881| 0.414194326714296|0.045161979893625165|\n",
      "|    min|                  0|                 0|                0.0|                  0|                 0|                 0|                 0|                 0|                   0|\n",
      "|    max|                  1|                 1|                1.0|                  1|                 1|                 1|                 1|                 1|                   1|\n",
      "+-------+-------------------+------------------+-------------------+-------------------+------------------+------------------+------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#match and miss dataframes based on is_match column \n",
    "matches = df.where(\"is_match = true\")\n",
    "matches_summary = matches.describe()\n",
    "\n",
    "misses = df.where(\"is_match = false\")\n",
    "misses_summary = misses.describe()\n",
    "\n",
    "matches_summary.show()\n",
    "misses_summary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain modified summaries\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "def modify_sum(summary):\n",
    "    #change names using pandas functions (transpose to make it visible)\n",
    "    summary_p = summary.toPandas()\n",
    "    summary_p = summary_p.set_index('summary').transpose().reset_index()\n",
    "    summary_p = summary_p.rename(columns = {'index': 'field'})\n",
    "    summary_p = summary_p.rename_axis(None, axis = 1)\n",
    "    print(summary_p)\n",
    "    #all columns are made into double type\n",
    "    summary_ss = ss.createDataFrame(summary_p)\n",
    "    for c in summary_ss.columns:\n",
    "        if c == 'field': \n",
    "            continue\n",
    "        summary_ss = summary_ss.withColumn(c, summary_ss[c].cast(DoubleType()))\n",
    "    summary_ss.printSchema()\n",
    "    return summary_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          field  count                mean                stddev  min  max\n",
      "0  cmp_fname_c1  20922  0.9973163859635041   0.03650667584833679    0    1\n",
      "1  cmp_fname_c2   1333  0.9898900320318176   0.08251973727615237    0    1\n",
      "2  cmp_lname_c1  20931  0.9970152595958817   0.04311880753394513  0.0  1.0\n",
      "3  cmp_lname_c2    475  0.9693701678438521   0.15345280740388917    0    1\n",
      "4       cmp_sex  20931   0.987291577086618   0.11201570591216435    0    1\n",
      "5        cmp_bd  20925  0.9970848267622461  0.053914876598079815    0    1\n",
      "6        cmp_bm  20925  0.9979450418160095  0.045286127452170664    0    1\n",
      "7        cmp_by  20925  0.9961290322580645   0.06209804856731054    0    1\n",
      "8       cmp_plz  20902  0.9584250310975027   0.19962063345931916    0    1\n",
      "root\n",
      " |-- field: string (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- mean: double (nullable = true)\n",
      " |-- stddev: double (nullable = true)\n",
      " |-- min: double (nullable = true)\n",
      " |-- max: double (nullable = true)\n",
      "\n",
      "          field    count                  mean                stddev  min  max\n",
      "0  cmp_fname_c1  5727203    0.7118634802167386   0.38908060096985375    0    1\n",
      "1  cmp_fname_c2   102365    0.8988473514090184    0.2727209029401023    0    1\n",
      "2  cmp_lname_c1  5728201    0.3131380113359623   0.33228121305726965  0.0  1.0\n",
      "3  cmp_lname_c2     1989   0.16295544855122535    0.1930236663528703    0    1\n",
      "4       cmp_sex  5728201    0.9548833918362851    0.2075598885921742    0    1\n",
      "5        cmp_bd  5727412    0.2216425149788421    0.4153518275558718    0    1\n",
      "6        cmp_bm  5727412     0.486995347986141    0.4998308940493881    0    1\n",
      "7        cmp_by  5727412    0.2199230647280133     0.414194326714296    0    1\n",
      "8       cmp_plz  5715387  0.002043781112285135  0.045161979893625165    0    1\n",
      "root\n",
      " |-- field: string (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- mean: double (nullable = true)\n",
      " |-- stddev: double (nullable = true)\n",
      " |-- min: double (nullable = true)\n",
      " |-- max: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "match_summary_ss = modify_sum(matches_summary)\n",
    "miss_summary_ss = modify_sum(misses_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+------------------+--------------------+---+---+\n",
      "|       field|  count|              mean|              stddev|min|max|\n",
      "+------------+-------+------------------+--------------------+---+---+\n",
      "|     cmp_sex|20931.0| 0.987291577086618| 0.11201570591216435|0.0|1.0|\n",
      "|cmp_lname_c1|20931.0|0.9970152595958817| 0.04311880753394513|0.0|1.0|\n",
      "|      cmp_bd|20925.0|0.9970848267622461|0.053914876598079815|0.0|1.0|\n",
      "|      cmp_by|20925.0|0.9961290322580645| 0.06209804856731054|0.0|1.0|\n",
      "|      cmp_bm|20925.0|0.9979450418160095|0.045286127452170664|0.0|1.0|\n",
      "|cmp_fname_c1|20922.0|0.9973163859635041| 0.03650667584833679|0.0|1.0|\n",
      "|     cmp_plz|20902.0|0.9584250310975027| 0.19962063345931916|0.0|1.0|\n",
      "|cmp_fname_c2| 1333.0|0.9898900320318176| 0.08251973727615237|0.0|1.0|\n",
      "|cmp_lname_c2|  475.0|0.9693701678438521| 0.15345280740388917|0.0|1.0|\n",
      "+------------+-------+------------------+--------------------+---+---+\n",
      "\n",
      "+------------+---------+--------------------+--------------------+---+---+\n",
      "|       field|    count|                mean|              stddev|min|max|\n",
      "+------------+---------+--------------------+--------------------+---+---+\n",
      "|cmp_lname_c1|5728201.0|  0.3131380113359623| 0.33228121305726965|0.0|1.0|\n",
      "|     cmp_sex|5728201.0|  0.9548833918362851|  0.2075598885921742|0.0|1.0|\n",
      "|      cmp_bm|5727412.0|   0.486995347986141|  0.4998308940493881|0.0|1.0|\n",
      "|      cmp_bd|5727412.0|  0.2216425149788421|  0.4153518275558718|0.0|1.0|\n",
      "|      cmp_by|5727412.0|  0.2199230647280133|   0.414194326714296|0.0|1.0|\n",
      "|cmp_fname_c1|5727203.0|  0.7118634802167386| 0.38908060096985375|0.0|1.0|\n",
      "|     cmp_plz|5715387.0|0.002043781112285135|0.045161979893625165|0.0|1.0|\n",
      "|cmp_fname_c2| 102365.0|  0.8988473514090184|  0.2727209029401023|0.0|1.0|\n",
      "|cmp_lname_c2|   1989.0| 0.16295544855122535|  0.1930236663528703|0.0|1.0|\n",
      "+------------+---------+--------------------+--------------------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#obtain top 5 columns with max count in both match and misses? \n",
    "match_summary_ss.sort(\"count\", ascending = False).show()\n",
    "miss_summary_ss.sort(\"count\", ascending = False).show()\n",
    "\n",
    "#based on the counts, choose top columns for similarity evaluation \n",
    "top_cols = [\"cmp_lname_c1\", \"cmp_sex\", \"cmp_by\", \"cmp_bd\", \"cmp_bm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|score|is_match|\n",
      "+-----+--------+\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  4.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "|  5.0|    true|\n",
      "+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Implement a PySpark program that computes similarity scores between records using a chosen similarity metric.\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "sum_exp = \"+\". join(top_cols)\n",
    "df = df.fillna(0, subset = top_cols)\n",
    "df_scored = df.withColumn('score', expr(sum_exp)).select('score', 'is_match')\n",
    "df_scored = df_scored.fillna(0)\n",
    "df_scored.show()\n",
    "#df_check = df_scored.groupBy('score').count()\n",
    "#df_check.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crosstabs(df_cop, num1): \n",
    "    return df_cop.selectExpr(f\"score >= {num1} as above\", \"is_match\").groupBy(\"above\").pivot(\"is_match\", (\"true\", \"false\")).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------+\n",
      "|above| true|  false|\n",
      "+-----+-----+-------+\n",
      "| true|20892| 160712|\n",
      "|false|   39|5567489|\n",
      "+-----+-----+-------+\n",
      "\n",
      "Precision:  0.11504151890927512\n",
      "False positive:  0.028758239640425374\n",
      "F1 score 0.023006934105859467\n"
     ]
    }
   ],
   "source": [
    "a = crosstabs(df_scored, 4.0)\n",
    "a.show()\n",
    "\n",
    "row_list = a.collect() \n",
    "precision = (row_list[0].__getitem__('true'))/(row_list[0].__getitem__('true') + row_list[0].__getitem__('false'))\n",
    "recall = (row_list[0].__getitem__('false'))/(row_list[0].__getitem__('true') + row_list[1].__getitem__('false'))\n",
    "print(\"Precision: \", precision)\n",
    "print(\"False positive: \", recall)\n",
    "print(\"F1 score\", (precision * recall)/(precision + recall))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
